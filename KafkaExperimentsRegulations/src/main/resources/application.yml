spring.application.name: regulations

server.port: ${REGULATIONS_SERVICE_PORT}

#endpoints.actuator.enabled: true
management.server.port: ${REGULATIONS_SERVICE_MANAGEMENT_PORT}
management:
  endpoints:
    web:
      base-path: /
      exposure.include: "*"
  endpoint:
    health:
      show-details: ALWAYS
#management.security.enabled: false
management.health.binders.enabled: true

eureka:
  client:
    registryFetchIntervalSeconds: 10
    serviceUrl.defaultZone: ${EUREKA_DEFAULT_ZONE_URL}
    healthcheck.enabled: true
  instance:
    leaseRenewalIntervalInSeconds: 10
    health-check-url-path: /health
    metadata-map:
      management:
        port: ${management.server.port}
        context-path: /

#spring.boot.admin.client.url: http://localhost:8080

info.app.name: Сервис работы с регламентами услуг (прототип)
info.app.description: Сервис работы с регламентами услуг (прототип)
info.app.version: 1.0.0

# Настройки Cloud Stream для Kafka Streams
spring.cloud.stream.kafka.streams.binder.configuration.application.server: localhost:${REGULATIONS_SERVICE_PORT}
spring.cloud.stream.kafka.binder.brokers: ${KAFKA_BOOTSTRAP_SERVERS}

spring.cloud.stream.kafka.streams.binder:
  brokers: ${KAFKA_BOOTSTRAP_SERVERS}
  zkNodes: ${KAFKA_ZK_NODES}
  configuration:
    commit.interval.ms: 1000
    schema.registry.url: ${KAFKA_SCHEMA_REGISTRY}
    auto.register.schemas: true
    specific.avro.reader: true
    max.schemas.per.subject: 1000
    value.subject.name.strategy: io.confluent.kafka.serializers.subject.TopicRecordNameStrategy
    default.deserialization.exception.handler: org.apache.kafka.streams.errors.LogAndContinueExceptionHandler

spring.cloud.stream.schemaRegistryClient.endpoint: ${KAFKA_SCHEMA_REGISTRY}
#spring.cloud.stream.schema.avro.dynamicSchemaGenerationEnabled: true

spring.cloud.stream.kafka.bindings.events-outgoing.producer:
  bufferSize: 0
  configuration:
    key.serializer: org.apache.kafka.common.serialization.StringSerializer
    value.serializer: io.confluent.kafka.streams.serdes.avro.SpecificAvroSerializer
    commit.interval.ms: 50
    metadata.max.age.ms: 10000
    batch.size: 0
    retention.ms: -1
    schema.registry.url: ${KAFKA_SCHEMA_REGISTRY}
    auto.register.schemas: true
    specific.avro.reader: true
    max.schemas.per.subject: 1000
    value.subject.name.strategy: io.confluent.kafka.serializers.subject.TopicRecordNameStrategy
    default.deserialization.exception.handler: org.apache.kafka.streams.errors.LogAndContinueExceptionHandler

spring.cloud.stream.kafka.bindings.events-projecting-outgoing.producer:
  bufferSize: 0
  configuration:
    commit.interval.ms: 50

spring.cloud.stream.kafka.streams:
  binder.configuration:
    state.dir: ${KAFKA_STATE_DIR:/tmp/kafka-state}
    default.key.serde: org.apache.kafka.common.serialization.Serdes$StringSerde
    #    default.value.serde: test.documents.service.DomainEventJsonSerde
    default.value.serde: io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde
  bindings.events-incoming.consumer:
    #    keySerde: test.cloud.streams.serde.PupilBaosSerde
    #    keySerde: org.apache.kafka.common.serialization.Serdes$StringSerde
    valueSerde: io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde
    application-id: ${spring.application.name}
  bindings.events-projecting-outgoing.producer:
    valueSerde: test.common.service.EventHolderSerde
  bindings.events-projecting-incoming.consumer:
    valueSerde: test.common.service.EventHolderSerde
    application-id: ${spring.application.name}

#spring.integration.readOnly.headers: contentType

spring.cloud.stream.bindings.events-incoming:
  destination: ${EVENTS_TOPIC_NAME}
  group: ${spring.application.name}
  consumer:
    partitioned: true
    useNativeDecoding: true
spring.cloud.stream.bindings.events-outgoing:
  destination: ${EVENTS_TOPIC_NAME}
  producer:
    partition-key-expression: headers['kafka_messageKey']
    useNativeEncoding: true
spring.cloud.stream.bindings.events-projecting-incoming:
  destination: ${SNAPSHOT_TOPIC_NAME}
  group: ${spring.application.name}
  consumer:
    partitioned: true
    useNativeDecoding: true
spring.cloud.stream.bindings.events-projecting-outgoing:
  destination: ${SNAPSHOT_TOPIC_NAME}
  producer:
    partition-key-expression: headers['kafka_messageKey']
    useNativeEncoding: true
spring.cloud.stream.bindings.events-quarantine-outgoing:
  destinations: events-quarantine
  producer:
    partition-key-expression: headers['kafka_messageKey']
    useNativeEncoding: true
spring.cloud.stream.bindings.error.destination: events-errors

spring.cloud.stream.bindings.events-errors:
  destination: events-errors



# Специфичные настройки приложения
kite.eventstore:
  kafka.localStore.name: ${STATE_STORE_NAME}
  mongo:
    dbAddress: mongodb://${MONGO_SERVER}
    database.name: ${MONGO_EVENTS_DB}
    username: ${MONGO_USER}
    password: ${MONGO_PASSWORD}
    authenticationDb: ${MONGO_AUTH_DB}

kite.mongo:
  dbAddress: mongodb://${MONGO_SERVER}
  database.name: ${MONGO_DB}
  username: ${MONGO_USER}
  password: ${MONGO_PASSWORD}
  authenticationDb: ${MONGO_AUTH_DB}


spring.aop.proxy-target-class: true

spring.jackson.mapper.DEFAULT_VIEW_INCLUSION: true

